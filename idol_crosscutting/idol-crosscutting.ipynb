{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import random\n",
    "\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from moviepy.editor import *\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법1: random distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDistance:\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        dur_end = min(reference_clip.duration, compare_clip.duration)\n",
    "        return random.randrange(1,100), min(dur_end, random.randrange(3,7)), {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법2: feature distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model의 FC layer를 추출한다.\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            *list(model.children())[:-1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDistance:\n",
    "    def __init__(self):\n",
    "        r3d_model = models.video.r3d_18(pretrained=True)\n",
    "        self.model = FeatureExtractor(r3d_model)\n",
    "        \n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        ref_frames = []\n",
    "        frames = []\n",
    "        for t in range(0, 10, 1):\n",
    "            ref_frames.append(cv2.resize(reference_clip.get_frame(t) / 255.0, (108, 192)))\n",
    "            frames.append(cv2.resize(compare_clip.get_frame(t) / 255.0, (108, 192)))\n",
    "\n",
    "        ref_frames = torch.from_numpy(np.array(ref_frames).reshape(-1, 3, 10, 108, 192)).float()\n",
    "        frames = torch.from_numpy(np.array(frames).reshape(-1, 3, 10, 108, 192)).float()\n",
    "\n",
    "        ref_feature = self.model(ref_frames)\n",
    "        feature = self.model(frames)\n",
    "\n",
    "        ret = ref_feature - feature\n",
    "        return np.mean(np.abs(ret.detach().numpy())), reference_clip.duration, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법3: Face distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_distance(ref_frame, frame):\n",
    "    face_detector = MTCNN(select_largest=True)\n",
    "    embed_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "    \n",
    "    ref_frame = np.array(ref_frame)\n",
    "    frame = np.array(frame)\n",
    "        \n",
    "    try:\n",
    "        ref_frame_detected = face_detector(ref_frame)\n",
    "        frame_detected = face_detector(frame)\n",
    "    except:\n",
    "        cosine_dist = 1\n",
    "        return cosine_dist\n",
    "    \n",
    "    ref_frame_embed = embed_model(ref_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "    frame_embed = embed_model(frame_detected.unsqueeze(0)).detach().numpy()\n",
    "    \n",
    "    ref_frame_embed = np.squeeze(ref_frame_embed)\n",
    "    frame_embed = np.squeeze(frame_embed)\n",
    "    \n",
    "    cosine_dist = 1 - np.dot(ref_frame_embed, frame_embed) / (np.linalg.norm(ref_frame_embed) * np.linalg.norm(frame_embed))\n",
    "    \n",
    "    return cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDistance:\n",
    "    def __init__(self, shape_predictor_path, face_embedding_penalty=None):\n",
    "        self.skip_frame_rate = 4\n",
    "        self.minimax_frames = 5\n",
    "        self.shape_predictor = shape_predictor_path\n",
    "        self.face_embedding_penalty = face_embedding_penalty\n",
    "        \n",
    "    def extract_landmark(self, reference_clip, compare_clip):\n",
    "        self.clips =[reference_clip, compare_clip]\n",
    "\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(self.shape_predictor)\n",
    "\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            i=0\n",
    "            every_frame_info= []\n",
    "            while True:\n",
    "                frame = clip.get_frame(i*1.0/clip.fps)\n",
    "                i+=self.skip_frame_rate\n",
    "                if (i*1.0/clip.fps)> clip.duration:\n",
    "                    break\n",
    "                \n",
    "                frame = imutils.resize(frame, width=800)\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray, 0)\n",
    "\n",
    "                if len(rects)>0:\n",
    "                    max_width = 0\n",
    "                    max_rect = None\n",
    "                    for rect in rects:\n",
    "                        if int(rects[0].width()) > max_width:\n",
    "                            max_rect = rect\n",
    "                    shape = predictor(gray, max_rect)\n",
    "                    shape = face_utils.shape_to_np(shape)\n",
    "                    every_frame_info.append(shape)\n",
    "                else:\n",
    "                    every_frame_info.append([])\n",
    "        \n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        return clips_frame_info\n",
    "\n",
    "\n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = []\n",
    "        # Calculate distance by frame\n",
    "        for i in range(min_size-1):\n",
    "            if len(clips_frame_info[0][i])>0 and len(clips_frame_info[1][i+1])>0: # 얼굴 둘다 있으면\n",
    "                # 두 영상에서 눈의 거리(왼쪽 눈 끼리, 오른쪽 눈 끼리)\n",
    "                l = 36 # 왼쪽 눈 왼쪽 끝\n",
    "                r = 45 # 오른쪽 눈3 오른쪽 끝\n",
    "                left_eye = ((clips_frame_info[0][i][l][0] - clips_frame_info[1][i+1][l][0])**2 + (clips_frame_info[0][i][l][1] - clips_frame_info[1][i+1][l][1])**2)**0.5\n",
    "                right_eye = ((clips_frame_info[0][i][r][0] - clips_frame_info[1][i+1][r][0])**2 + (clips_frame_info[0][i][r][1] - clips_frame_info[1][i+1][r][1])**2)**0.5\n",
    "                total_diff = left_eye + right_eye\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        time.sleep(2.0)\n",
    "        clips_frame_info = self.extract_landmark(reference_clip, compare_clip) # 모든 프레임마다 길이 계산해줌\n",
    "        min_size = min(len(clips_frame_info[0]),len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "        clips =[reference_clip,compare_clip]\n",
    "\n",
    "        # Minimize max distance in (minimax_frames) frames\n",
    "        minimax_frames = self.minimax_frames\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0\n",
    "        max_dist = []\n",
    "        for i in range(min_size - (minimax_frames - 1)): # 해당 frame \"이전과 이후\"에 frame들 확인\n",
    "            start_minmax_idx = 0 if (i - minimax_frames)<0 else i - minimax_frames\n",
    "            if (None in dist_arr[start_minmax_idx :i + minimax_frames]):\n",
    "                max_dist.append(None)\n",
    "            else:\n",
    "                tmp_max = np.max(dist_arr[start_minmax_idx:i + minimax_frames])\n",
    "                max_dist.append(tmp_max)\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "        \n",
    "        # face embedding penalty\n",
    "        if self.face_embedding_penalty is not None:\n",
    "            ref_frame = reference_clip.get_frame(min_idx)\n",
    "            frame = compare_clip.get_frame(min_idx)\n",
    "\n",
    "            cosine_dist = calculate_cosine_distance(ref_frame, frame)\n",
    "            print(\"Face Panelty Applied With \", cosine_dist * self.face_embedding_penalty)\n",
    "            min_diff += cosine_dist * self.face_embedding_penalty\n",
    "        \n",
    "        # return distance, second, additional_info\n",
    "        return min_diff, (min_idx*self.skip_frame_rate)/self.clips[0].fps, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법4: pose distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDistance:\n",
    "    def __init__(self):\n",
    "        self.SKIP_FRAME_RATE = 10\n",
    "        self.MINIMAX_FRAME = 4\n",
    "        # 함수에서 documentaiton 읽기\n",
    "        self.model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.model.eval()\n",
    "        os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "    def extract_boxes(self, reference_clip, compare_clip):\n",
    "        self.clips = [reference_clip, compare_clip]\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            i = 0\n",
    "            every_frame_info = []\n",
    "            # loop over the frames from the video stream\n",
    "            while True:\n",
    "                i+=self.SKIP_FRAME_RATE # 1초에 60 fps가 있으므로 몇개는 skip해도 될거 같음!\n",
    "                if (i*1.0/clip.fps)> clip.duration:\n",
    "                    break\n",
    "\n",
    "                frame = clip.get_frame(i*1.0/clip.fps)\n",
    "                frame = imutils.resize(frame, width=640)\n",
    "                frame = frame/255 # image, and should be in ``0-1`` range.\n",
    "                frame = np.transpose(frame, (2,0,1)) #  HWC -> CHW(그 위치에 몇차원 애를 넣을거냔?)\n",
    "                x = [torch.from_numpy(frame).float()]\n",
    "                # label list https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt\n",
    "                predictions = self.model(x)\n",
    "                prediction= predictions[0]\n",
    "                each_box_list = zip(prediction['boxes'].tolist(), prediction['labels'].tolist(), prediction['scores'].tolist())\n",
    "                # 0.95 정도 올려야 까맣게 보이는 관중이 없어짐!\n",
    "                filtered_box_list = filter(lambda x: x[1]==1 and x[2] >= 0.95, each_box_list)\n",
    "                filtered_center_dot_list = list(map(lambda x: [(x[0][0]+x[0][2])/2, (x[0][1]+x[0][3])/2], filtered_box_list))\n",
    "                # x좌표로 정렬하기(대형이 가로로 늘어져 있다고 가정하고 순서대로 정렬)\n",
    "                sorted_dot_list = sorted(filtered_center_dot_list, key = lambda x: x[0])\n",
    "                every_frame_info.append(sorted_dot_list) # 프레임별 정보\n",
    "            \n",
    "            clips_frame_info.append(np.array(every_frame_info)) # 각 영상별로 붙이기\n",
    "\n",
    "        return clips_frame_info\n",
    "\n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = list()\n",
    "        # Calculate distance (by frame)\n",
    "        for i in range(min_size):\n",
    "            if len(clips_frame_info[0][i])>0 and len(clips_frame_info[1][i])>0: # 둘다 있으면\n",
    "                # x축 값이 가장 가까운걸로 찾고 그거랑 비교(어차피 대형이 중요한거니까)\n",
    "                ref_frame_dots = clips_frame_info[0][i] # 해당 frame의 정보\n",
    "                compare_frame_dots = clips_frame_info[1][i] # 해당 frame의 정보\n",
    "                min_dot_num = min(len(ref_frame_dots), len(compare_frame_dots)) # reference 기준으로 계산할거양\n",
    "                dot_num_diff = abs(len(ref_frame_dots)- len(compare_frame_dots))\n",
    "                penalty = ((self.clips[0].w **2 + self.clips[0].h**2)**0.5) * abs(len(ref_frame_dots)-len(compare_frame_dots)) # 개수가 다를때 주는 패널티\n",
    "                total_diff = penalty * dot_num_diff\n",
    "                for dot_idx in range(min_dot_num):\n",
    "                    total_diff += ((ref_frame_dots[dot_idx][0] - compare_frame_dots[dot_idx][0])**2 + (ref_frame_dots[dot_idx][1] - compare_frame_dots[dot_idx][1])**2)**0.5\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        clips_frame_info = self.extract_boxes(reference_clip, compare_clip) # 모든 프레임마다 길이 계산해줌\n",
    "        min_size = min(len(clips_frame_info[0]),len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)  \n",
    "\n",
    "        # Minimize max distance in (minimax_frames) frames\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0 \n",
    "        max_dist = []\n",
    "        for i in range(min_size-(self.MINIMAX_FRAME-1)):\n",
    "            if None in dist_arr[i:i+self.MINIMAX_FRAME]:\n",
    "                max_dist.append(None)\n",
    "            else:\n",
    "                tmp_max = np.max(dist_arr[i:i+self.MINIMAX_FRAME])\n",
    "                max_dist.append(tmp_max)\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "\n",
    "        # return distance, second, additional_info\n",
    "        return min_diff, (min_idx*self.SKIP_FRAME_RATE)/reference_clip.fps, {}\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            *list(model.children())[:-1] # models?\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crosscut:\n",
    "    def __init__(self, dist_obj, video_path, output_path):\n",
    "        self.videos_path = video_path # vieo\"들\"이 있는 path\n",
    "        self.output_path = output_path\n",
    "        self.min_time = 1000.0 # 가장 짧은 영상의 길이(INIT)\n",
    "        video_num = len(os.listdir(self.videos_path))\n",
    "        self.start_times = [0] * video_num\n",
    "        self.window_time = 10\n",
    "        self.padded_time = 4 # padding time\n",
    "        self.dist_obj = dist_obj\n",
    "        self.audioclip = None # 기준으로 사용할 audio\n",
    "        self.extracted_clips_array = []\n",
    "    \n",
    "    def video_alignment(self):\n",
    "        # VIDEO ALIGNMENT -> SLICE START TIME\n",
    "        for i in range(len(os.listdir(self.videos_path))):\n",
    "            video_path = os.path.join(self.videos_path, sorted(os.listdir(self.videos_path))[i])\n",
    "            clip = VideoFileClip(video_path)\n",
    "            clip = clip.subclip(self.start_times[i], clip.duration)\n",
    "            if self.min_time > clip.duration:\n",
    "                self.audioclip = clip.audio\n",
    "                self.min_time = clip.duration\n",
    "            self.extracted_clips_array.append(clip)\n",
    "        print('LOGGER-- {} Video Will Be Mixed'.format(len(self.extracted_clips_array)))\n",
    "\n",
    "\n",
    "    def generate_video(self):\n",
    "        self.video_alignment()\n",
    "        # CONCAT SUBCLIP 0~ MIN DURATION CLIP TIME\n",
    "        extracted_clips_array = self.extracted_clips_array\n",
    "        con_clips = [] # Staged Mixed Clip array\n",
    "        t = 3 # 초반 3초 INIT\n",
    "        current_idx = 0 # INIT\n",
    "        \n",
    "        con_clips.append(extracted_clips_array[current_idx].subclip(0, min(t, int(self.min_time)))) #초반 padding\n",
    "        while t < int(self.min_time):\n",
    "            # Cut 10 sec.\n",
    "            cur_t = t\n",
    "            next_t = min(t+self.window_time, self.min_time)\n",
    "\n",
    "            reference_clip = extracted_clips_array[current_idx].subclip(cur_t, next_t)\n",
    "            d = float(\"Inf\")\n",
    "            cur_clip = None\n",
    "            min_idx = (current_idx+1)%len(extracted_clips_array) # 거리가 무한일 때 최소한 나는 선택하지 않도록 하기\n",
    "            for video_idx in range(len(extracted_clips_array)):\n",
    "                if video_idx == current_idx:\n",
    "                    continue\n",
    "                clip = extracted_clips_array[video_idx].subclip(cur_t, next_t) \n",
    "                \n",
    "                cur_d, plus_frame, additional_info = self.dist_obj.distance(reference_clip, clip) \n",
    "                print(current_idx, video_idx, cur_d, cur_t + plus_frame)\n",
    "                if d > cur_d:\n",
    "                    d = cur_d\n",
    "                    min_idx = video_idx\n",
    "                    next_t = cur_t + plus_frame # 바로 옮길 frame\n",
    "                    cur_clip = reference_clip.subclip(0, plus_frame)\n",
    "            if cur_clip: \n",
    "                clip = cur_clip \n",
    "            else:\n",
    "                clip = reference_clip\n",
    "            t = next_t\n",
    "            con_clips.append(clip)\n",
    "\n",
    "            # 다음 clip : padding 길이는 반드시 append\n",
    "            current_idx = min_idx \n",
    "            print(\"idx : {}\".format(current_idx))\n",
    "            pad_clip = extracted_clips_array[current_idx].subclip(t, min(self.min_time,t+self.padded_time))\n",
    "            t = min(self.min_time,t + self.padded_time)\n",
    "            con_clips.append(pad_clip)\n",
    "\n",
    "        final_clip = concatenate_videoclips(con_clips)\n",
    "\n",
    "        if self.audioclip !=None:\n",
    "            print(\"Not None\")\n",
    "            final_clip.audio = self.audioclip\n",
    "\n",
    "        final_clip.write_videofile(self.output_path)\n",
    "        return final_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_stagemix.mp4\n",
      "LOGGER-- 3 Video Will Be Mixed\n",
      "0 1 483.34414474952894 8.605599999999999\n",
      "0 2 inf 3.0\n",
      "idx : 1\n",
      "1 0 inf 12.605599999999999\n",
      "1 2 inf 12.605599999999999\n",
      "idx : 2\n",
      "2 0 inf 26.6056\n",
      "2 1 inf 26.6056\n",
      "idx : 0\n",
      "0 1 592.5767613230869 45.01\n",
      "0 2 inf 40.605599999999995\n",
      "idx : 1\n",
      "1 0 521.498360649854 55.14946666666666\n",
      "1 2 inf 49.01\n",
      "idx : 0\n",
      "0 1 534.3428338834478 59.14946666666666\n",
      "0 2 inf 59.14946666666666\n",
      "idx : 1\n",
      "Not None\n",
      "chunk:   7%|▋         | 98/1324 [00:00<00:01, 965.18it/s, now=None]Moviepy - Building video my_stagemix.mp4.\n",
      "MoviePy - Writing audio in my_stagemixTEMP_MPY_wvf_snd.mp3\n",
      "t:   1%|▏         | 23/1800 [00:00<00:07, 228.28it/s, now=None]MoviePy - Done.\n",
      "Moviepy - Writing video my_stagemix.mp4\n",
      "\n",
      "Moviepy - Done !\n",
      "Moviepy - video ready my_stagemix.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<moviepy.video.VideoClip.VideoClip at 0x7fb2cb723c70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method = 'pose'\n",
    "video_path = './fifth_season'\n",
    "output_path = 'my_stagemix.mp4'\n",
    "shape_predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "face_embedding_penalty = 100 # or None\n",
    "\n",
    "print(output_path)\n",
    "# ~.py --method random\n",
    "if method == 'random':\n",
    "    random_distance = RandomDistance()\n",
    "    cross_cut = Crosscut(random_distance, video_path, output_path)\n",
    "elif method == 'face':\n",
    "    face_distance = FaceDistance(shape_predictor_path, face_embedding_penalty)\n",
    "    cross_cut = Crosscut(face_distance, video_path, output_path)\n",
    "elif method == 'pose':\n",
    "    pose_distance = PoseDistance()\n",
    "    cross_cut = Crosscut(pose_distance, video_path, output_path)\n",
    "elif method == 'feature':\n",
    "    feature_distance = FeatureDistance()\n",
    "    cross_cut = Crosscut(feature_distance, video_path, output_path)\n",
    "cross_cut.generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

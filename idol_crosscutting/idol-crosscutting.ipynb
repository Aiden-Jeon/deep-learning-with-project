{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import random\n",
    "\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from moviepy.editor import *\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법1: Random distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDistance:\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        dur_end = min(reference_clip.duration, compare_clip.duration)\n",
    "        return random.randrange(1,100), min(dur_end, random.randrange(3,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법2: Feature distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            *list(model.children())[:-1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDistance:\n",
    "    def __init__(self):\n",
    "        r3d_model = models.video.r3d_18(pretrained=True)\n",
    "        self.model = FeatureExtractor(r3d_model)\n",
    "        \n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        ref_frames = []\n",
    "        frames = []\n",
    "        for t in range(0, 10, 1):\n",
    "            ref_frames.append(cv2.resize(reference_clip.get_frame(t) / 255.0, (108, 192)))\n",
    "            frames.append(cv2.resize(compare_clip.get_frame(t) / 255.0, (108, 192)))\n",
    "\n",
    "        ref_frames = torch.from_numpy(np.array(ref_frames).reshape(-1, 3, 10, 108, 192)).float()\n",
    "        frames = torch.from_numpy(np.array(frames).reshape(-1, 3, 10, 108, 192)).float()\n",
    "\n",
    "        ref_feature = self.model(ref_frames)\n",
    "        feature = self.model(frames)\n",
    "\n",
    "        ret = ref_feature - feature\n",
    "        return np.mean(np.abs(ret.detach().numpy())), reference_clip.duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법3: Face distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDistance:\n",
    "    def __init__(self, shape_predictor_path, face_embedding_penalty=None):\n",
    "        self.skip_frame_rate = 4\n",
    "        self.minimax_frames = 5\n",
    "        self.shape_predictor = shape_predictor_path\n",
    "        self.face_embedding_penalty = face_embedding_penalty\n",
    "        \n",
    "    def extract_landmark(self, reference_clip, compare_clip):\n",
    "        self.clips =[reference_clip, compare_clip]\n",
    "\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(self.shape_predictor)\n",
    "\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            i=0\n",
    "            every_frame_info= []\n",
    "            while True:\n",
    "                frame = clip.get_frame(i*1.0/clip.fps)\n",
    "                i+=self.skip_frame_rate\n",
    "                if (i*1.0/clip.fps)> clip.duration:\n",
    "                    break\n",
    "                \n",
    "                frame = imutils.resize(frame, width=800)\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray, 0)\n",
    "\n",
    "                if len(rects)>0:\n",
    "                    max_width = 0\n",
    "                    max_rect = None\n",
    "                    for rect in rects:\n",
    "                        if int(rects[0].width()) > max_width:\n",
    "                            max_rect = rect\n",
    "                    shape = predictor(gray, max_rect)\n",
    "                    shape = face_utils.shape_to_np(shape)\n",
    "                    every_frame_info.append(shape)\n",
    "                else:\n",
    "                    every_frame_info.append([])\n",
    "        \n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        return clips_frame_info\n",
    "    \n",
    "    def embedding_cosine_distance(self, reference_frame, compare_frame):\n",
    "        face_detector = MTCNN(select_largest=True)\n",
    "        embed_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        \n",
    "        reference_frame = np.array(reference_frame)\n",
    "        compare_frame = np.array(compare_frame)\n",
    "        try:\n",
    "            reference_frame_detected = face_detector(reference_frame)\n",
    "            compare_frame_detected = face_detector(compare_frame)\n",
    "        except:\n",
    "            cosine_dist = 1\n",
    "            return cosine_dist\n",
    "        \n",
    "        reference_frame_embed = embed_model(reference_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        compare_frame_embed = embed_model(compare_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        reference_frame_embed = np.squeeze(reference_frame_embed)\n",
    "        compare_frame_embed = np.squeeze(compare_frame_embed)\n",
    "\n",
    "        cosine_dist = 1 - np.dot(reference_frame_embed, compare_frame_embed) / (np.linalg.norm(reference_frame_embed) * np.linalg.norm(compare_frame_embed))\n",
    "        return cosine_dist\n",
    "\n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = []\n",
    "        for i in range(min_size-1):\n",
    "            if len(clips_frame_info[0][i])>0 and len(clips_frame_info[1][i+1])>0:\n",
    "                l = 36\n",
    "                r = 45\n",
    "                left_eye = ((clips_frame_info[0][i][l][0] - clips_frame_info[1][i+1][l][0])**2 + (clips_frame_info[0][i][l][1] - clips_frame_info[1][i+1][l][1])**2)**0.5\n",
    "                right_eye = ((clips_frame_info[0][i][r][0] - clips_frame_info[1][i+1][r][0])**2 + (clips_frame_info[0][i][r][1] - clips_frame_info[1][i+1][r][1])**2)**0.5\n",
    "                total_diff = left_eye + right_eye\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        clips_frame_info = self.extract_landmark(reference_clip, compare_clip)\n",
    "        min_size = min(len(clips_frame_info[0]),len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "        clips =[reference_clip,compare_clip]\n",
    "\n",
    "        minimax_frames = self.minimax_frames\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0\n",
    "        for i in range(min_size - (minimax_frames - 1)):\n",
    "            start_minmax_idx = 0 if (i - minimax_frames)<0 else i - minimax_frames\n",
    "            if (None not in dist_arr[start_minmax_idx :i + minimax_frames]):\n",
    "                tmp_max = np.max(dist_arr[start_minmax_idx:i + minimax_frames])\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "        \n",
    "        if self.face_embedding_penalty != None and min_diff < np.float(\"Inf\"):\n",
    "            ref_frame = reference_clip.get_frame(min_idx * 1.0/reference_clip.fps)\n",
    "            frame = compare_clip.get_frame(min_idx * 1.0/compare_clip.fps)\n",
    "\n",
    "            cosine_dist = self.embedding_cosine_distance(ref_frame, frame)\n",
    "            min_diff += cosine_dist * self.face_embedding_penalty\n",
    "        \n",
    "        return min_diff, (min_idx*self.skip_frame_rate)/self.clips[0].fps\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법4: Pose distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDistance:\n",
    "    def __init__(self):\n",
    "        self.SKIP_FRAME_RATE = 10\n",
    "        self.MINIMAX_FRAME = 4\n",
    "        self.model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.model.eval()\n",
    "        os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "    def extract_boxes(self, reference_clip, compare_clip):\n",
    "        self.clips = [reference_clip, compare_clip]\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            i = 0\n",
    "            every_frame_info = []\n",
    "            while True:\n",
    "                i+=self.SKIP_FRAME_RATE \n",
    "                if (i*1.0/clip.fps)> clip.duration:\n",
    "                    break\n",
    "\n",
    "                frame = clip.get_frame(i*1.0/clip.fps)\n",
    "                frame = imutils.resize(frame, width=640)\n",
    "                frame = frame/255\n",
    "                frame = np.transpose(frame, (2,0,1))\n",
    "                x = [torch.from_numpy(frame).float()]\n",
    "                predictions = self.model(x)\n",
    "                prediction= predictions[0]\n",
    "                each_box_list = zip(prediction['boxes'].tolist(), prediction['labels'].tolist(), prediction['scores'].tolist())\n",
    "                filtered_box_list = filter(lambda x: x[1]==1 and x[2] >= 0.95, each_box_list)\n",
    "                filtered_center_dot_list = list(map(lambda x: [(x[0][0]+x[0][2])/2, (x[0][1]+x[0][3])/2], filtered_box_list))\n",
    "                sorted_dot_list = sorted(filtered_center_dot_list, key = lambda x: x[0])\n",
    "                every_frame_info.append(sorted_dot_list)\n",
    "            \n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "\n",
    "        return clips_frame_info\n",
    "\n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = list()\n",
    "        for i in range(min_size):\n",
    "            if len(clips_frame_info[0][i])>0 and len(clips_frame_info[1][i])>0:\n",
    "                ref_frame_dots = clips_frame_info[0][i]\n",
    "                compare_frame_dots = clips_frame_info[1][i]\n",
    "                min_dot_num = min(len(ref_frame_dots), len(compare_frame_dots))\n",
    "                dot_num_diff = abs(len(ref_frame_dots)- len(compare_frame_dots))\n",
    "                penalty = ((self.clips[0].w **2 + self.clips[0].h**2)**0.5) * abs(len(ref_frame_dots)-len(compare_frame_dots)) \n",
    "                total_diff = penalty * dot_num_diff\n",
    "                for dot_idx in range(min_dot_num):\n",
    "                    total_diff += ((ref_frame_dots[dot_idx][0] - compare_frame_dots[dot_idx][0])**2 + (ref_frame_dots[dot_idx][1] - compare_frame_dots[dot_idx][1])**2)**0.5\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        clips_frame_info = self.extract_boxes(reference_clip, compare_clip)\n",
    "        min_size = min(len(clips_frame_info[0]),len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)  \n",
    "\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0 \n",
    "\n",
    "        for i in range(min_size-(self.MINIMAX_FRAME-1)):\n",
    "            start_minmax_idx = 0 if (i - self.MINIMAX_FRAME)<0 else i - self.MINIMAX_FRAME\n",
    "            if (None not in dist_arr[start_minmax_idx :i + self.MINIMAX_FRAME]):\n",
    "                tmp_max = np.max(dist_arr[i:i+self.MINIMAX_FRAME])\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "\n",
    "        return min_diff, (min_idx*self.SKIP_FRAME_RATE)/reference_clip.fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crosscut:\n",
    "    def __init__(self, dist_obj, video_path, output_path):\n",
    "        self.videos_path = video_path\n",
    "        self.output_path = output_path\n",
    "        self.min_time = 1000.0\n",
    "        video_num = len(os.listdir(self.videos_path))\n",
    "        self.start_times = [0] * video_num\n",
    "        self.window_time = 10\n",
    "        self.padded_time = 4\n",
    "        self.dist_obj = dist_obj\n",
    "        self.audioclip = None\n",
    "        self.extracted_clips_array = []\n",
    "        self.con_clips = []\n",
    "    \n",
    "    def video_alignment(self):\n",
    "        for i in range(len(os.listdir(self.videos_path))):\n",
    "            video_path = os.path.join(self.videos_path, sorted(os.listdir(self.videos_path))[i])\n",
    "            clip = VideoFileClip(video_path)\n",
    "            clip = clip.subclip(self.start_times[i], clip.duration)\n",
    "            if self.min_time > clip.duration:\n",
    "                self.audioclip = clip.audio\n",
    "                self.min_time = clip.duration\n",
    "            self.extracted_clips_array.append(clip)\n",
    "        print('LOGGER-- {} Video Will Be Mixed'.format(len(self.extracted_clips_array)))\n",
    "    \n",
    "    def select_next_clip(self, t, current_idx):\n",
    "        cur_t = t\n",
    "        next_t = min(t+self.window_time, self.min_time)\n",
    "        \n",
    "        reference_clip = self.extracted_clips_array[current_idx].subclip(cur_t, next_t)\n",
    "        d = float(\"Inf\")\n",
    "        cur_clip = None\n",
    "        min_idx = (current_idx+1)%len(self.extracted_clips_array)\n",
    "        for video_idx in range(len(self.extracted_clips_array)):\n",
    "            if video_idx == current_idx:\n",
    "                continue\n",
    "            clip = self.extracted_clips_array[video_idx].subclip(cur_t, next_t) \n",
    "\n",
    "            cur_d, plus_frame = self.dist_obj.distance(reference_clip, clip) \n",
    "            print(current_idx, video_idx, cur_d, cur_t + plus_frame)\n",
    "            if d > cur_d:\n",
    "                d = cur_d\n",
    "                min_idx = video_idx\n",
    "                next_t = cur_t + plus_frame\n",
    "                cur_clip = reference_clip.subclip(0, plus_frame)\n",
    "        \n",
    "        if cur_clip: \n",
    "            clip = cur_clip \n",
    "        else:\n",
    "            clip = reference_clip\n",
    "        self.con_clips.append(clip)\n",
    "        t = next_t\n",
    "        return t, min_idx\n",
    "        \n",
    "    def add_padding(self, t, next_idx):\n",
    "        print(\"idx : {}\".format(next_idx))\n",
    "        pad_clip = self.extracted_clips_array[next_idx].subclip(t, min(self.min_time,t+self.padded_time))\n",
    "        self.con_clips.append(pad_clip)\n",
    "        \n",
    "        t = min(self.min_time,t + self.padded_time)\n",
    "        return t, next_idx    \n",
    "        \n",
    "    def write_video(self):\n",
    "        final_clip = concatenate_videoclips(self.con_clips)\n",
    "        if self.audioclip != None:\n",
    "            print(\"Not None\")\n",
    "            final_clip.audio = self.audioclip\n",
    "        final_clip.write_videofile(self.output_path)\n",
    "        return final_clip    \n",
    "    \n",
    "    def generate_video(self):\n",
    "        self.video_alignment()\n",
    "        t = 3\n",
    "        current_idx = 0\n",
    "\n",
    "        self.con_clips.append(self.extracted_clips_array[current_idx].subclip(0, min(t, int(self.min_time))))\n",
    "        while t < int(self.min_time):\n",
    "            t, min_idx = self.select_next_clip(t, current_idx)\n",
    "            t, current_idx = self.add_padding(t, min_idx)\n",
    "\n",
    "        final_clip = self.write_video()\n",
    "        return final_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'face'\n",
    "video_path = 'fifth_season'\n",
    "output_path = 'my_stagemix.mp4'\n",
    "shape_predictor_path = 'shape_predictor_68_face_landmarks.dat'\n",
    "face_embedding_penalty = 100 # or None\n",
    "\n",
    "print(output_path)\n",
    "if method == 'random':\n",
    "    random_distance = RandomDistance()\n",
    "    cross_cut = Crosscut(random_distance, video_path, output_path)\n",
    "elif method == 'face':\n",
    "    face_distance = FaceDistance(shape_predictor_path, face_embedding_penalty)\n",
    "    cross_cut = Crosscut(face_distance, video_path, output_path)\n",
    "elif method == 'pose':\n",
    "    pose_distance = PoseDistance()\n",
    "    cross_cut = Crosscut(pose_distance, video_path, output_path)\n",
    "elif method == 'feature':\n",
    "    feature_distance = FeatureDistance()\n",
    "    cross_cut = Crosscut(feature_distance, video_path, output_path)\n",
    "cross_cut.generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('cross_cut_pose': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0b12dddfa8203924054b3e6dc811275fe46e16d276269db8ab96fd1c01b3af8f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}